## 인프런_딥러닝 강의

@섹션0~3.
- (hypothesis, cost function, gradient descent algo.)
- hupothesis = Wx + b // W, b 두개의 값을 학습
- cost(W,b) = 실제값과 예측한 값의 차이를 제곱하여 평균한 것(밥그릇 모양)
- 밥그릇 모양의 그래프에서 최적의 값을 찾기 위한 알고리즘 : gradient descent algorithm 

@섹션4. 여러개의 입력의 Linear Regression

- 이전까지는 하나의 인풋을 통해 학습했지만, 여러개의 input은 어떻게 해야할가?
- 하나의 input일땐 h=Wx+b, 3개의 입력일 경우 H(x1, x2, x3) = w1x1 + w2x2 + w3x3 + b
- cost function은 동일함
- 더 많은 경우(Multi-variable)의 경우도 위와 같이 동일한 형태를 띔

- 그러나 이를 잘 처리하기 위해 Matrix를 이용함. 행렬곱셈을 이용하면 쉽게 표현할 수 있음
- 1행3열의 행렬 (x1 x2 x3)와 3행1열의 행렬(w1 w2 w3)을 곱하면 (x1w1+x2w2+x3w3) 표현이 가능, H(X)=XW // x1w1 과 w1x1은 같으므로 순서 상관이 없음

- 인스턴스(x의 데이터 행 하나하나)가 많아도 동일함.
- 5개의 인스턴스인 x input 3개의 경우를 예를 들어보면, [5,3]행렬과 [3,1]행렬의 곱으로 [5,1] 5by1 결과가 도출됨.
- H(X)= XW,  X [5, 3] // 5개의 인스턴스 3개의 input
- [n,3] [3,1] = [n,1] // XW = H(X), 행렬의 곱셈 결과 형식을 이해해두자!
- ex) [n,3] [?,?] = [n,2]의 경우 곱셈결과 형식에 따라 3by2임을 알 수 있다.
-Lecture(theory)에서는 H(x) = Wx + b라면 텐서플로우에서는 H(X) = XW 형태로 구현된다. 수학적의미는 동일함을 알아두자.

@섹션5. Binary Classification
- 둘 중 하나의 결과를 도출시키는 classification
- 예시는 페이스북 피드 업로드, 스팸메일 처리, 신용카드 오사용 분류
- 이러한 예시를 컴퓨터로 쉽게 코드를 짤 수 있도록 encoding을 함, 0과 1로 구분짓는 것임. 스팸메일(1)_정상메일(0)
- 0과 1로 이루어진 결과만 도출하는 상황에서 linear regression으로 해결하는 것은 cost의 값이 매우 커지게 됨.
- 이유인즉 1이상의 값, 0이하의 값을 linear regression에서 도출되기 때문임
- 이 cost의 값을 낮추기 위한 그래프, 시그모이드(logistic fuction)라 함.
- H(x) = z = Wx+b, g(z) -> 0~1사이의 결과만 나오게 됨

- logistic function을 최소화 하기 위한 cost fuction.
- cost함수에 적용시 굽이친 그래프가 나오게 되며, local minimum을 global minimum을 찾지 못하게 하므로 y가 1인 상황과 0인 상황으로 나누며 로그 함수를 이용한 cost function을 적용함. 
- -log(H(x) : y=1 // -log(1-H(x)) : y=0
- 위 식을 하나의 식으로 통합할 수 있음.
- 텐서플로우에서 위 cost함수를 표현하고 gradientdescentoptimizer를 이용해서 적용하면 됨.

- tensorflow에서 데이터 읽기.
- numpy를 통해 대규모 다차원 배열을 쉽게 처리할 수 있게 해주는 파이썬 라이브러리 사용.
- import numpy as np
- np.loadtxt(csv 파일을 불러옴, 데이터를 나누는 기준, 데이터 타입)
- python Slicing : 시퀀스 자료구조의 특정 데이터를 잘라내는 것
- [:, 0:-1] 앞에 [ : ] 는 전체 행을 의미, [0:-1]은 0번 인덱스부터 -1번 인덱스까지 // -1은 마지막 원소 전까지
- [-1]이면 마지막 원소 이전것만 적용

- placeholder에서 shape=[None, 3] 코드를 볼 수 있다. none은 데이터 갯수를 초반에 모를 경우 이후에 임의로 바뀔 수 있다는 것임

- Queue Runners : 데이터가 커서 메모리에 올리기 힘든 경우 Queue Runners를 사용함. 여러 파일을 큐에 쌓아 놓고 reader로 파일을 읽어옴
- 과정 : 1. 파일 이름을 리스트로 생성 2. 파일 읽어올 리더 선언, 연결 3. value 파싱 설정(디코딩)

@ 세션6. Softmax Regression
- logistic regression 복습
- H(x) = Wx의 결과가 1을 초과한 큰 값이 나오므로 이를 z로 생각하고, g(z)라는 수식이 큰 값들을 0과 1사이로 압축해줌. 
- x -> w -> z -> y의 햇 // z를 통해 시그모이드로 변환, y햇은 0과 1사이
- 2개의 결과를 구분 짓은 리니어를 그리는 것.

- multinomial classification ( 여러개의 입력을 통한 여러개의 결과 )
- binary classification으로 multinomial classification 구현이 가능함.
- ex) a, b, c가 있는 경우 a b / c - c or not, b / a c - b or not, a / b c - a or not
- y --s(y)--> p(모두 합하면 1, 확률) <-one-hot enncoding-> 하나의 유력한 값이 1로 변함. 
- cross-entropy cost function 




@ 세션7 Learning rate, Evaluation
- learning rate를 잘못 설정할 경우.
- test data를 통해 모델을 다시 한번 재 평가하여 확인해주어야 함.
- training과 test datasets을 나누어야 함. 모델의 정확도를 파악해야 하기 때문임
- training : 모델에 학습되는 데이터 / test datasets : 모델이 한번도 보지 못한 데이터
- learning rate가 너무 크면, 움직이는 폭이 커서 밖으로 튀어나가는 overshooting이 발생할 수 있음
- learning rate가 너무 작으면 학습이 너무 느려서 진행이 안될 수 있으며, 작은 굴곡이 존재하면 그 안에서 갇히는 문제가 발생 - local minima 현상
- 이 문제를 normalization 정규화를 통해 0~1사이의 값을 매겨주는 일반화 과정을 거침.
- 데이터 크기가 들쭉날쭉할때는 일반화 과정을 하는 것이 좋음

- MNIST Dataset : 우편번호를 자동으로 읽을 수 있도록 만듬




















