### 인프런_딥러닝 강의

@섹션0~3.
- (hypothesis, cost function, gradient descent algo.)
- hupothesis = Wx + b // W, b 두개의 값을 학습
- cost(W,b) = 실제값과 예측한 값의 차이를 제곱하여 평균한 것(밥그릇 모양)
- 밥그릇 모양의 그래프에서 최적의 값을 찾기 위한 알고리즘 : gradient descent algorithm 

@섹션4. 여러개의 입력의 Linear Regression

- 이전까지는 하나의 인풋을 통해 학습했지만, 여러개의 input은 어떻게 해야할가?
- 하나의 input일땐 h=Wx+b, 3개의 입력일 경우 H(x1, x2, x3) = w1x1 + w2x2 + w3x3 + b
- cost function은 동일함
- 더 많은 경우(Multi-variable)의 경우도 위와 같이 동일한 형태를 띔

- 그러나 이를 잘 처리하기 위해 Matrix를 이용함. 행렬곱셈을 이용하면 쉽게 표현할 수 있음
- 1행3열의 행렬 (x1 x2 x3)와 3행1열의 행렬(w1 w2 w3)을 곱하면 (x1w1+x2w2+x3w3) 표현이 가능, H(X)=XW // x1w1 과 w1x1은 같으므로 순서 상관이 없음

- 인스턴스(x의 데이터 행 하나하나)가 많아도 동일함.
- 5개의 인스턴스인 x input 3개의 경우를 예를 들어보면, [5,3]행렬과 [3,1]행렬의 곱으로 [5,1] 5by1 결과가 도출됨.
- H(X)= XW,  X [5, 3] // 5개의 인스턴스 3개의 input
- [n,3] [3,1] = [n,1] // XW = H(X), 행렬의 곱셈 결과 형식을 이해해두자!
- ex) [n,3] [?,?] = [n,2]의 경우 곱셈결과 형식에 따라 3by2임을 알 수 있다.
-Lecture(theory)에서는 H(x) = Wx + b라면 텐서플로우에서는 H(X) = XW 형태로 구현된다. 수학적의미는 동일함을 알아두자.
